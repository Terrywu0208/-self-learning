{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特徵工程 : 特徵提取 => 字典資料轉成電腦看得懂的 one-hot編碼\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "data = [{\"city\":\"A\",\"temp\":20},{\"city\":\"B\",\"temp\":40},{\"city\":\"C\",\"temp\":60}]\n",
    "\n",
    "#實例化類 DictVectorize()\n",
    "#調用fit_transform方法輸入數據並轉換\n",
    "transfor = DictVectorizer(sparse=False) #sparse=False 返回二街矩陣 ; sparse=True 返回sparse matrix 稀疏矩陣\n",
    "\n",
    "data_new = transfor.fit_transform(data) #轉換成sparse matrix (稀疏矩陣是為了減少儲存空見 顯示非零的位置及值)\n",
    "\n",
    "print(\"DATA: \\n\",data_new)\n",
    "print(\"特徵名稱 \\n\", transfor.get_feature_names()) #transfor.get_feature_names() 回傳特徵名稱\n",
    "# [A,B,C,temp]對照著\n",
    "\n",
    "#應用場景:\n",
    "    # 1) 數據集中類別特徵表較多\n",
    "        # 1.將樹聚集的特徵轉換成字典類型\n",
    "        # 2.DictVectorizer專換成 one-hot \n",
    "    # 2) 本身那道的數據就是字典類型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特徵工程 : 特徵提取 => 文本資料轉成電腦看得懂的 one-hot編碼\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "data = [\"from sklearn sklearn feature_extraction text import CountVectorizer\",\"from sklearn feature_extraction import DictVectorizer\"]\n",
    "\n",
    "transfor = CountVectorizer() #將CountVectorizer() 實例化 , 沒有 sparse 參數\n",
    "\n",
    "data_new = transfor.fit_transform(data)\n",
    "\n",
    "print(data_new.toarray()) #將sparse matrix 轉換成二為矩陣可以用 => xx.toarray() , 回傳的為詞出現的個數\n",
    "print(transfor.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 特徵工程 : 特徵提取 => 文本資料轉成電腦看得懂的 one-hot編碼 (透過結巴斷詞處理中文)\n",
    "import jieba\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "data = \"即使他只是參與這段對話，都有可能改變全局，因為簡單來說，我們可以回答他的問題，我們可以提出一個明確的計畫。他所詢問的問題，我們都很樂意回答。我很期待與他討論，因為生命危在旦夕\"\n",
    "cut_data = [\" \".join(list(jieba.cut(data)))] #use jieba to tansform the data, which contain chinese to indiviual word\n",
    "transfor = CountVectorizer(stop_words=[\"一個\"])  #stop_words 可以設定不要那些文字 (型態字串)\n",
    "data_new = transfor.fit_transform(cut_data) #傳入的data 型態需要是字典\n",
    "\n",
    "print(data_new)\n",
    "print(transfor.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache C:\\Users\\User\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.727 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.12935311 0.         0.         0.         0.         0.\n",
      "  0.1700838  0.         0.         0.         0.1700838  0.1700838\n",
      "  0.         0.         0.1700838  0.12935311 0.25870622 0.12935311\n",
      "  0.34016759 0.34016759 0.25870622 0.         0.         0.\n",
      "  0.1700838  0.         0.         0.         0.         0.\n",
      "  0.38805933 0.         0.         0.         0.         0.1700838\n",
      "  0.1700838  0.         0.         0.         0.         0.1700838\n",
      "  0.1700838  0.         0.         0.         0.         0.1700838\n",
      "  0.         0.         0.         0.1700838  0.         0.\n",
      "  0.         0.         0.1700838  0.1700838  0.         0.1700838\n",
      "  0.         0.         0.         0.         0.1700838  0.\n",
      "  0.         0.        ]\n",
      " [0.11555848 0.15194551 0.15194551 0.         0.         0.\n",
      "  0.         0.15194551 0.15194551 0.15194551 0.         0.\n",
      "  0.         0.         0.         0.         0.11555848 0.11555848\n",
      "  0.         0.         0.11555848 0.15194551 0.         0.15194551\n",
      "  0.         0.15194551 0.15194551 0.15194551 0.         0.\n",
      "  0.         0.15194551 0.15194551 0.15194551 0.         0.\n",
      "  0.         0.15194551 0.15194551 0.60778204 0.         0.\n",
      "  0.         0.         0.15194551 0.15194551 0.         0.\n",
      "  0.         0.15194551 0.15194551 0.         0.         0.\n",
      "  0.         0.15194551 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.15194551\n",
      "  0.30389102 0.        ]\n",
      " [0.         0.         0.         0.51572434 0.12893109 0.12893109\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.12893109 0.12893109 0.         0.09805541 0.         0.\n",
      "  0.         0.         0.         0.         0.12893109 0.\n",
      "  0.         0.         0.         0.         0.12893109 0.12893109\n",
      "  0.09805541 0.         0.         0.         0.12893109 0.\n",
      "  0.         0.         0.         0.         0.38679326 0.\n",
      "  0.         0.12893109 0.         0.         0.12893109 0.\n",
      "  0.12893109 0.         0.         0.         0.12893109 0.38679326\n",
      "  0.25786217 0.         0.         0.         0.25786217 0.\n",
      "  0.12893109 0.12893109 0.12893109 0.12893109 0.         0.\n",
      "  0.         0.12893109]]\n"
     ]
    }
   ],
   "source": [
    "## 特徵工程 : 特徵提取 => 文本資料轉成電腦看得懂的 one-hot編碼 (透過結巴斷詞處理中文) => 利用 tfidf 進行特徵抽取, 能夠有效找到關鍵字\n",
    "## 對文章分類更有效\n",
    "\n",
    "import jieba \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def cut_word(text_data): #利用結巴轉換成單詞\n",
    "    cut_data = \" \".join(list(jieba.cut(text_data)))\n",
    "    return cut_data\n",
    "\n",
    "\n",
    "data = [\"即使他只是參與這段對話，都有可能改變全局，因為簡單來說，我們可以回答他的問題，我們可以提出一個明確的計畫。他所詢問的問題，我們都很樂意回答。我很期待與他討論，因為生命危在旦夕\",\n",
    "        \"找到文件當中的關鍵字。怎樣的關鍵字是重要的？一個直覺的想法是出現最多次的字。這可能可以，不過因為每個文件的字數不同，無法比較。所以在用文件內的字數作為分母，將所有文件得到數值加以規範化\",\n",
    "        \"據原作者的說法，使用預設詞庫的話，繁體中文的斷詞結果應該會比較差，畢竟原來的詞庫是簡體中文，但在這個例子中，我感覺是繁體中文的斷詞結果比較好，這應該只是特例，我們接下來試試看中文歌詞的斷詞結果如何\"]\n",
    "\n",
    "data_new = []\n",
    "\n",
    "for i in data:\n",
    "    text_new = cut_word(i)\n",
    "    data_new.append(text_new)\n",
    "\n",
    "transfor = TfidfVectorizer()\n",
    "data_new_filter = transfor.fit_transform(data_new)\n",
    "print(data_new_filter.toarray()) #回傳值為tfidf指標 越高代表越重要 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特徵預處理\n",
    "\n",
    "# 無量綱化（nondimensionalize 或者dimensionless）是指通過一個合適的變數替代，將一個涉及物理量的方程的部分或全部的單位移除，以求簡化實驗或者計算的目的，是科學研究中一種重要的處理思想。\n",
    "# 無量鋼化 => 歸一化 、 標準化\n",
    "#歸一化 => 將數據映射到某區間 eg: 0 ~ 1之間 \n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "\n",
    "raw_data = pd.read_csv(\"xxx.cvs\")\n",
    "data = raw_data.iloc[:,:3] #取出想使用的資料\n",
    "\n",
    "# 實例化轉換器\n",
    "\n",
    "transfor = MinMaxScaler()\n",
    "data_new = transfor.fit_transform(data)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2647ea34e536f865ab67ff9ddee7fd78773d956cec0cab53c79b32cd10da5d83"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
